{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuNvxxICo90XPp6Ni0aEym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiyer31/TensorFlow-Tutorials/blob/master/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ3JUcdP1vbX"
      },
      "source": [
        "#Aiming to try to write by own basic CNN to predict images in the MNIST dataset.\n",
        "\n",
        "This is based off of Pedersen's TF tutorials. Also at my github.\n",
        "https://github.com/aiyer31/TensorFlow-Tutorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBeGHfdZ2B-H"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNfFhirW2EUD"
      },
      "source": [
        "Each image is 28x28. \n",
        "\n",
        "Layer 1: Convolution of 5x5. Using 5x5 because the intuition is that a tapering kernel size will capture the general features followed by specific features. **Is this true?** We can use 16 of these filters so that we end up with a set of 16 feature maps. Zero pad the 28x28 image (SAME convolution). We'll go with the \"same\" convolution technique rather than a full/valid convolution. To pool these images, we can use a 2x2 max pooling. Each of the feature maps will then become size 14x14.\n",
        "\n",
        "Layer 2: Another convolutional layer wherein, we use 32 3x3 filters and pad accordingly. 2x2 max pooling further reduces the size of each feature map to 7x7. \n",
        "\n",
        "Side note: When the filter size decreases we are using more feature maps/channels.\n",
        "\n",
        "Layer 3: Flatten the layer and feed into a fully connected layer. The size of the input to the layer based on the above numbers is going to be:\n",
        "num_feature maps*(input_image_size/pooling_filter_size) = 32 * (14 * 14 / 2 * 2) = 32 * 7 * 7 = 1568\n",
        "\n",
        "The number of hidden units in the FC layer can be 128.\n",
        "\n",
        "Layer 4: Softmax layer with 10 classes for prediction\n",
        "\n",
        "The loss function will be softmax cross entropy with L2 regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsAXQkzi13sJ"
      },
      "source": [
        "### Imports\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y9SQyd65MNB"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import math"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo_zxWrQ5NB4"
      },
      "source": [
        "# Use TensorFlow v.2 with this old v.1 code.\n",
        "# E.g. placeholder variables and sessions have changed in TF2.\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2NQgB7d55Oxd",
        "outputId": "29a98889-dc8a-4d5a-8fd9-d1b2f44e5262"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3aZjHRD5SoU"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Dc4Yuy5sFT"
      },
      "source": [
        "# Clone the repository from GitHub to Google Colab's temporary drive.\n",
        "import os\n",
        "work_dir = \"/content/TensorFlow-Tutorials/\"\n",
        "if not os.path.exists(work_dir):\n",
        "    !git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git\n",
        "os.chdir(work_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUujMNOL5WMA",
        "outputId": "ebdeacd6-f0b7-476c-805c-66c5b5d506e6"
      },
      "source": [
        "from mnist import MNIST\n",
        "data = MNIST(data_dir=\"data/MNIST/\")\n",
        "\n",
        "# The number of pixels in each dimension of an image.\n",
        "img_dim = data.img_size\n",
        "\n",
        "# The images are stored in one-dimensional arrays of this length.\n",
        "img_size_flat = data.img_size_flat\n",
        "\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = data.img_shape\n",
        "\n",
        "# Number of classes, one class for each of 10 digits.\n",
        "num_classes = data.num_classes\n",
        "\n",
        "# Number of colour channels for the images: 1 channel for gray-scale.\n",
        "num_channels = data.num_channels\n",
        "data.num_train, data.num_test, img_dim, img_size_flat, img_shape, num_classes, num_channels"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 10000, 28, 784, (28, 28), 10, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g21xI483WijJ"
      },
      "source": [
        "# Convert the raw data into TF tensors\n",
        "\n",
        "# We define placeholders for inputs so that for every iteration of training this placeholder changes value. This is why we can't use tf.constant to feed our inputs into tensorflow.\n",
        "input = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='input')\n",
        "\n",
        "# The convolutional layers expect x to be a 4 dimensional tensor -> (batch size, image_height, image_width, num_input_channels)\n",
        "# In our example, image_height = image_width, batch_size is inferred so we pass -1 and num_input_channels is defined by num_channels above.\n",
        "input_image = tf.reshape(input, shape=[-1, img_dim, img_dim, num_channels])\n",
        "\n",
        "# As for the output label we have:\n",
        "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"y_true\")\n",
        "\n",
        "y_true_class = tf.argmax(y_true, axis=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I2vGgRXHIun"
      },
      "source": [
        "## Helper functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGgWtc-pWBRW"
      },
      "source": [
        "def get_weights(layer_name, layer_shape):\n",
        "  return tf.compat.v1.get_variable(f\"{layer_name}-weights\", shape=layer_shape, dtype=tf.float32, initializer=tf.compat.v1.truncated_normal_initializer())\n",
        "\n",
        "def get_biases(layer_name, layer_length):\n",
        "  return tf.compat.v1.get_variable(f\"{layer_name}-biases\", shape=[layer_length], dtype=tf.float32)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIi9BRA4Hdmd"
      },
      "source": [
        "### Helper function that creates a fully connected layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UShZOhSYHlPw"
      },
      "source": [
        "# This function instantiates a fully connected layer of size \n",
        "# *layer_size* with input *input*. The activation function used is passed via a string, defaulting to RELU.\n",
        "# The function also takes in \"input_name\" which is a string to denote the weights and biases of for the FC layer.\n",
        "\n",
        "_RELU = \"relu\"\n",
        "_SOFTMAX = \"softmax\"\n",
        "def activation_function(activation_str: str, input_tensor):\n",
        "  if activation_str == _RELU:\n",
        "      return tf.nn.relu(input_tensor)\n",
        "  if activation_str == _SOFTMAX:\n",
        "      return tf.nn.softmax(input_tensor)\n",
        "  return input_tensor # no activation\n",
        "\n",
        "def fc_layer(input_name, input, input_size, layer_size, activation_str = _RELU):\n",
        "  weights = get_weights(input_name, layer_shape=[input_size, layer_size])\n",
        "  biases = get_biases(input_name, layer_length=layer_size)\n",
        "\n",
        "  return activation_function(activation_str, tf.matmul(input, weights) + biases)\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-NG3ZXLvOk"
      },
      "source": [
        "## Helper function that creates a convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLMUNYDMLyKN"
      },
      "source": [
        "# The convolutional layer takes in:\n",
        "# 1. Input\n",
        "# 2. Number of input feature maps\n",
        "# 3. Number of kernel filters\n",
        "# 4. Kernel width (Assuming only square filters)\n",
        "\n",
        "def convolutional_layer(layer_name, input, num_input_feature_maps, kernel_width, num_kernels):\n",
        "  kernel_weights = get_weights(layer_name, layer_shape=[kernel_width, kernel_width, num_input_feature_maps, num_kernels])\n",
        "  kernel_biases = get_biases(layer_name, layer_length=num_kernels)\n",
        "\n",
        "  # Now that we've initialized the kernels, we need to convolve the kernel with the input\n",
        "  # We can use the tf.nn.conv2d function. The filters argument needs to be of size [filter_width, filter_width, num_in_channels, num_out_channels]\n",
        "  convolved_image = tf.nn.conv2d(input=input, filters=kernel_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "  convolved_image += kernel_biases\n",
        "\n",
        "  # Now that we've convolved the image we need to decide how we would like to pool.\n",
        "  # For now let us use 2x2 max-pooling\n",
        "  # TODO: make pooling optional, and pooling arguments optional to this function.\n",
        "  pooled_image = tf.nn.max_pool(value=convolved_image, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "  # Apply activation on the pooled image\n",
        "  # NOTE: Usually activations happen before pooling but max_pool and RELU are commutative anyway\n",
        "  return tf.nn.relu(pooled_image)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6AwAKIyY1id"
      },
      "source": [
        "## Helpful Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWS4BOPLSc79"
      },
      "source": [
        "conv_1_filter_size = 5\n",
        "conv_1_num_filters = 16\n",
        "\n",
        "conv_2_filter_size = 3\n",
        "conv_2_num_filters = 32\n",
        "\n",
        "fc_layer_size = 128\n",
        "\n",
        "batch_size = 64"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FIPczz2U3Ka"
      },
      "source": [
        "## Layer 1: The convolutional layer using 16 5x5 filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OCcXUg9VLFk"
      },
      "source": [
        "layer_1 = convolutional_layer(layer_name=\"convolution_1\", \n",
        "                              input=input_image, \n",
        "                              num_input_feature_maps=num_channels,\n",
        "                              kernel_width=conv_1_filter_size,\n",
        "                              num_kernels=conv_1_num_filters\n",
        "                              )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxe_p5mmZYol",
        "outputId": "54310974-92af-414d-c73b-dcfbe25c371e"
      },
      "source": [
        "# Checking the size of layer_1\n",
        "layer_1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Relu:0' shape=(?, 14, 14, 16) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHSNWFG5ZNC2"
      },
      "source": [
        "## Layer 2: Convolutional layer using 32 3x3 filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOF7EkWsZMNS"
      },
      "source": [
        "layer_2 = convolutional_layer(layer_name=\"convolution_2\", \n",
        "                              input=layer_1, \n",
        "                              num_input_feature_maps=conv_1_num_filters,\n",
        "                              kernel_width=conv_2_filter_size,\n",
        "                              num_kernels=conv_2_num_filters\n",
        "                              )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K33NTIt6agtG",
        "outputId": "89357e31-84b1-40d4-84d8-78bec926f6d2"
      },
      "source": [
        "layer_2"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Relu_1:0' shape=(?, 7, 7, 32) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn8qOX70cCUL"
      },
      "source": [
        "## Flatten the input to prepare for the fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu2Mi239cFm9",
        "outputId": "f5ad21e6-837c-4301-b1bc-9d558cf00e2b"
      },
      "source": [
        "layer_shape = layer_2.get_shape()\n",
        "layer_shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(32)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOfHddSrcpQn",
        "outputId": "6791e3b3-41cd-4688-ddf9-56777ff78109"
      },
      "source": [
        "layer_shape[0:4]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(32)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anl21iQHctXf",
        "outputId": "7b448bc5-8fa1-4e89-9d04-3eb4b9346511"
      },
      "source": [
        "layer_shape[1:4]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(7), Dimension(7), Dimension(32)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9a_mCODcu8x",
        "outputId": "d208990e-cbb8-4d46-fef0-7cd766621513"
      },
      "source": [
        "layer_shape[1:4].num_elements()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qHL9brDczdz",
        "outputId": "4a734cc1-7951-4e12-b9f2-432a181b8aea"
      },
      "source": [
        "num_features = layer_2.get_shape()[1:4].num_elements() # The shape is assumed to be a 4 dimensional tensor looking like (batch_size, img_height, img_width, num_channels)\n",
        "flattened_layer = tf.reshape(layer_2, shape=[-1, num_features])\n",
        "flattened_layer"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Reshape_2:0' shape=(?, 1568) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrVmNqUAb4N-"
      },
      "source": [
        "## Layer 3: Fully connected Layer with 128 hidden units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zro1ic1Sahec",
        "outputId": "32e4dc08-59b7-4277-8a02-03220dbf0d8e"
      },
      "source": [
        "fully_connected_layer = fc_layer(input_name=\"dense_layer\", input=flattened_layer, input_size=num_features, layer_size=fc_layer_size)\n",
        "fully_connected_layer"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQc1SZG8dcLZ"
      },
      "source": [
        "final_prediction_layer = fc_layer(input_name=\"prediction_layer\", input=fully_connected_layer, input_size=fc_layer_size, layer_size=num_classes, activation_str=None)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b6O31HrebsL",
        "outputId": "aa63f245-c0b1-44ce-9fff-4750de787d64"
      },
      "source": [
        "final_prediction_layer"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'add_3:0' shape=(?, 10) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOUz5lRaefTH"
      },
      "source": [
        "## Predict the class by taking the argmax of the softmax over the above layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywxWG9EuecjR"
      },
      "source": [
        "y_pred = tf.nn.softmax(final_prediction_layer)\n",
        "y_pred_class = tf.argmax(y_pred, axis=1)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_42OZ6Fye3wK"
      },
      "source": [
        "## Define the loss function for this network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qt1qO25e2Rj",
        "outputId": "faa0f64d-e1b7-4d93-dbea-c95304a71d3f"
      },
      "source": [
        "# We'll be using the softmax cross entropy loss (just like binary cross entropy/log loss but with multiple classes)\n",
        "\n",
        "cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=final_prediction_layer,\n",
        "                                                             labels=y_true) # Apparently the softmax function calculates the softmax internally, so we must use the predictions PRE-softmax application.\n",
        "\n",
        "cost = tf.reduce_mean(cross_entropy_loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xqmf92zfuTu"
      },
      "source": [
        "## Define the optimization method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPK5a1gnfimq"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv4dIahFgsZz"
      },
      "source": [
        "## Performance Measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7sm7hMyf1XD"
      },
      "source": [
        "# We'll just use accuracy for now. Might be worthwhile to understand if there's any data skew in the above dataset.\n",
        "correct_prediction = tf.equal(y_pred_class, y_true_class)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toshiviZhQW5"
      },
      "source": [
        "## Setting TF up to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F5ak3Jag_lS"
      },
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ORybu6shcBp"
      },
      "source": [
        "# Counter for total number of iterations performed so far.\n",
        "total_iterations = 0\n",
        "\n",
        "def optimize(num_iterations):\n",
        "    # Ensure we update the global variable rather than a local copy.\n",
        "    global total_iterations\n",
        "\n",
        "    # Start-time used for printing time-usage below.\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(total_iterations,\n",
        "                   total_iterations + num_iterations):\n",
        "\n",
        "        # Get a batch of training examples.\n",
        "        # x_batch now holds a batch of images and\n",
        "        # y_true_batch are the true labels for those images.\n",
        "        x_batch, y_true_batch, _ = data.random_batch(batch_size=batch_size)\n",
        "\n",
        "        # Put the batch into a dict with the proper names\n",
        "        # for placeholder variables in the TensorFlow graph.\n",
        "        feed_dict_train = {input: x_batch,\n",
        "                           y_true: y_true_batch}\n",
        "\n",
        "        # Run the optimizer using this batch of training data.\n",
        "        # TensorFlow assigns the variables in feed_dict_train\n",
        "        # to the placeholder variables and then runs the optimizer.\n",
        "        session.run(optimizer, feed_dict=feed_dict_train)\n",
        "\n",
        "        # Print status every 100 iterations.\n",
        "        if i % 100 == 0:\n",
        "            # Calculate the accuracy on the training-set.\n",
        "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
        "\n",
        "            # Message for printing.\n",
        "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
        "\n",
        "            # Print it.\n",
        "            print(msg.format(i + 1, acc))\n",
        "\n",
        "    # Update the total number of iterations performed.\n",
        "    total_iterations += num_iterations\n",
        "\n",
        "    # Ending time.\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Difference between start and end-times.\n",
        "    time_dif = end_time - start_time\n",
        "\n",
        "    # Print the time-usage.\n",
        "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSlJbWwBhqhC"
      },
      "source": [
        "### Function to plot example errors:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdTNFVwjhkOQ"
      },
      "source": [
        "def plot_example_errors(cls_pred, correct):\n",
        "    # This function is called from print_test_accuracy() below.\n",
        "\n",
        "    # cls_pred is an array of the predicted class-number for\n",
        "    # all images in the test-set.\n",
        "\n",
        "    # correct is a boolean array whether the predicted class\n",
        "    # is equal to the true class for each image in the test-set.\n",
        "\n",
        "    # Negate the boolean array.\n",
        "    incorrect = (correct == False)\n",
        "    \n",
        "    # Get the images from the test-set that have been\n",
        "    # incorrectly classified.\n",
        "    images = data.x_test[incorrect]\n",
        "    \n",
        "    # Get the predicted classes for those images.\n",
        "    cls_pred = cls_pred[incorrect]\n",
        "\n",
        "    # Get the true classes for those images.\n",
        "    cls_true = data.y_test_cls[incorrect]\n",
        "    \n",
        "    # Plot the first 9 images.\n",
        "    plot_images(images=images[0:9],\n",
        "                cls_true=cls_true[0:9],\n",
        "                cls_pred=cls_pred[0:9])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8FlRJK8hte3"
      },
      "source": [
        "### Function to plot confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ydfVi0hwke"
      },
      "source": [
        "def plot_confusion_matrix(cls_pred):\n",
        "    # This is called from print_test_accuracy() below.\n",
        "\n",
        "    # cls_pred is an array of the predicted class-number for\n",
        "    # all images in the test-set.\n",
        "\n",
        "    # Get the true classifications for the test-set.\n",
        "    cls_true = data.y_test_cls\n",
        "    \n",
        "    # Get the confusion matrix using sklearn.\n",
        "    cm = confusion_matrix(y_true=cls_true,\n",
        "                          y_pred=cls_pred)\n",
        "\n",
        "    # Print the confusion matrix as text.\n",
        "    print(cm)\n",
        "\n",
        "    # Plot the confusion matrix as an image.\n",
        "    plt.matshow(cm)\n",
        "\n",
        "    # Make various adjustments to the plot.\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(num_classes)\n",
        "    plt.xticks(tick_marks, range(num_classes))\n",
        "    plt.yticks(tick_marks, range(num_classes))\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JreXuJvPh3wi"
      },
      "source": [
        "### Function to print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNgoL03XhzLo"
      },
      "source": [
        "# Split the test-set into smaller batches of this size.\n",
        "test_batch_size = 256\n",
        "\n",
        "def print_test_accuracy(show_example_errors=False,\n",
        "                        show_confusion_matrix=False):\n",
        "\n",
        "    # Number of images in the test-set.\n",
        "    num_test = data.num_test\n",
        "\n",
        "    # Allocate an array for the predicted classes which\n",
        "    # will be calculated in batches and filled into this array.\n",
        "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
        "\n",
        "    # Now calculate the predicted classes for the batches.\n",
        "    # We will just iterate through all the batches.\n",
        "    # There might be a more clever and Pythonic way of doing this.\n",
        "\n",
        "    # The starting index for the next batch is denoted i.\n",
        "    i = 0\n",
        "\n",
        "    while i < num_test:\n",
        "        # The ending index for the next batch is denoted j.\n",
        "        j = min(i + test_batch_size, num_test)\n",
        "\n",
        "        # Get the images from the test-set between index i and j.\n",
        "        images = data.x_test[i:j, :]\n",
        "\n",
        "        # Get the associated labels.\n",
        "        labels = data.y_test[i:j, :]\n",
        "\n",
        "        # Create a feed-dict with these images and labels.\n",
        "        feed_dict = {input: images,\n",
        "                     y_true: labels}\n",
        "\n",
        "        # Calculate the predicted class using TensorFlow.\n",
        "        cls_pred[i:j] = session.run(y_pred_class, feed_dict=feed_dict)\n",
        "\n",
        "        # Set the start-index for the next batch to the\n",
        "        # end-index of the current batch.\n",
        "        i = j\n",
        "\n",
        "    # Convenience variable for the true class-numbers of the test-set.\n",
        "    cls_true = data.y_test_cls\n",
        "\n",
        "    # Create a boolean array whether each image is correctly classified.\n",
        "    correct = (cls_true == cls_pred)\n",
        "\n",
        "    # Calculate the number of correctly classified images.\n",
        "    # When summing a boolean array, False means 0 and True means 1.\n",
        "    correct_sum = correct.sum()\n",
        "\n",
        "    # Classification accuracy is the number of correctly classified\n",
        "    # images divided by the total number of images in the test-set.\n",
        "    acc = float(correct_sum) / num_test\n",
        "\n",
        "    # Print the accuracy.\n",
        "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
        "    print(msg.format(acc, correct_sum, num_test))\n",
        "\n",
        "    # Plot some examples of mis-classifications, if desired.\n",
        "    if show_example_errors:\n",
        "        print(\"Example errors:\")\n",
        "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
        "\n",
        "    # Plot the confusion matrix, if desired.\n",
        "    if show_confusion_matrix:\n",
        "        print(\"Confusion Matrix:\")\n",
        "        plot_confusion_matrix(cls_pred=cls_pred)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaTM3BFoh6M6"
      },
      "source": [
        "### Accuracy before training:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofpqwm1ih5Vx",
        "outputId": "3c8a541b-80ff-4be8-b271-fef82ad550fd"
      },
      "source": [
        "print_test_accuracy()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test-Set: 9.9% (991 / 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9iex7qkI1Z"
      },
      "source": [
        "### Accuracy After One Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKqAO0ith_RW",
        "outputId": "5e8f454b-7a62-4585-ebed-3c63c6c48b3f"
      },
      "source": [
        "optimize(num_iterations=1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization Iteration:      1, Training Accuracy:   7.8%\n",
            "Time usage: 0:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVa1Ui2NkQ9n",
        "outputId": "9b3d6811-6bf6-43a7-dfb7-6126d76b39f1"
      },
      "source": [
        "print_test_accuracy()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test-Set: 10.0% (1000 / 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDFwko6AkeEW"
      },
      "source": [
        "### Accuracy After 100 optimization iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FTr7aeNkVFN",
        "outputId": "3c64aba3-27da-4b6c-b891-31ffc803fc93"
      },
      "source": [
        "optimize(num_iterations=99)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time usage: 0:00:04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2QGih33kkV9",
        "outputId": "e17fd051-6349-481f-c1b8-8a94eba8b232"
      },
      "source": [
        "print_test_accuracy()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test-Set: 15.5% (1551 / 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46MAxzDxksu4"
      },
      "source": [
        "### Accuracy after 1000 iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpe9W-oKkrpI",
        "outputId": "55282ff3-bb17-4fa3-b077-b638dfb02180"
      },
      "source": [
        "optimize(num_iterations=900)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization Iteration:    101, Training Accuracy:  12.5%\n",
            "Optimization Iteration:    201, Training Accuracy:  17.2%\n",
            "Optimization Iteration:    301, Training Accuracy:  32.8%\n",
            "Optimization Iteration:    401, Training Accuracy:  31.2%\n",
            "Optimization Iteration:    501, Training Accuracy:  48.4%\n",
            "Optimization Iteration:    601, Training Accuracy:  39.1%\n",
            "Optimization Iteration:    701, Training Accuracy:  51.6%\n",
            "Optimization Iteration:    801, Training Accuracy:  57.8%\n",
            "Optimization Iteration:    901, Training Accuracy:  65.6%\n",
            "Time usage: 0:00:34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCisLqgkmzj",
        "outputId": "a2fa6da3-2a0d-4464-f9a1-7fab4ea78758"
      },
      "source": [
        "print_test_accuracy()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test-Set: 66.5% (6653 / 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2mJiZKblBkb"
      },
      "source": [
        "### Performance after 10000 iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lgwAs8Kkzy7",
        "outputId": "dc0b0ce0-35d7-45e4-db7c-879f5a8c5fb9"
      },
      "source": [
        "optimize(num_iterations=9000)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization Iteration:   1001, Training Accuracy:  56.2%\n",
            "Optimization Iteration:   1101, Training Accuracy:  75.0%\n",
            "Optimization Iteration:   1201, Training Accuracy:  71.9%\n",
            "Optimization Iteration:   1301, Training Accuracy:  67.2%\n",
            "Optimization Iteration:   1401, Training Accuracy:  70.3%\n",
            "Optimization Iteration:   1501, Training Accuracy:  76.6%\n",
            "Optimization Iteration:   1601, Training Accuracy:  81.2%\n",
            "Optimization Iteration:   1701, Training Accuracy:  75.0%\n",
            "Optimization Iteration:   1801, Training Accuracy:  85.9%\n",
            "Optimization Iteration:   1901, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   2001, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   2101, Training Accuracy:  78.1%\n",
            "Optimization Iteration:   2201, Training Accuracy:  85.9%\n",
            "Optimization Iteration:   2301, Training Accuracy:  84.4%\n",
            "Optimization Iteration:   2401, Training Accuracy:  82.8%\n",
            "Optimization Iteration:   2501, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   2601, Training Accuracy:  84.4%\n",
            "Optimization Iteration:   2701, Training Accuracy:  75.0%\n",
            "Optimization Iteration:   2801, Training Accuracy:  76.6%\n",
            "Optimization Iteration:   2901, Training Accuracy:  79.7%\n",
            "Optimization Iteration:   3001, Training Accuracy:  81.2%\n",
            "Optimization Iteration:   3101, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   3201, Training Accuracy:  82.8%\n",
            "Optimization Iteration:   3301, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   3401, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   3501, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   3601, Training Accuracy:  85.9%\n",
            "Optimization Iteration:   3701, Training Accuracy:  78.1%\n",
            "Optimization Iteration:   3801, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   3901, Training Accuracy:  75.0%\n",
            "Optimization Iteration:   4001, Training Accuracy:  84.4%\n",
            "Optimization Iteration:   4101, Training Accuracy:  75.0%\n",
            "Optimization Iteration:   4201, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   4301, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   4401, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   4501, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   4601, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   4701, Training Accuracy:  82.8%\n",
            "Optimization Iteration:   4801, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   4901, Training Accuracy:  84.4%\n",
            "Optimization Iteration:   5001, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   5101, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   5201, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   5301, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   5401, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   5501, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   5601, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   5701, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   5801, Training Accuracy:  85.9%\n",
            "Optimization Iteration:   5901, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   6001, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   6101, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   6201, Training Accuracy:  82.8%\n",
            "Optimization Iteration:   6301, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   6401, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   6501, Training Accuracy: 100.0%\n",
            "Optimization Iteration:   6601, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   6701, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   6801, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   6901, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   7001, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   7101, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   7201, Training Accuracy:  96.9%\n",
            "Optimization Iteration:   7301, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   7401, Training Accuracy:  85.9%\n",
            "Optimization Iteration:   7501, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   7601, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   7701, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   7801, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   7901, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   8001, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   8101, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   8201, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   8301, Training Accuracy:  98.4%\n",
            "Optimization Iteration:   8401, Training Accuracy:  82.8%\n",
            "Optimization Iteration:   8501, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   8601, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   8701, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   8801, Training Accuracy:  89.1%\n",
            "Optimization Iteration:   8901, Training Accuracy:  87.5%\n",
            "Optimization Iteration:   9001, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   9101, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   9201, Training Accuracy:  96.9%\n",
            "Optimization Iteration:   9301, Training Accuracy:  93.8%\n",
            "Optimization Iteration:   9401, Training Accuracy:  90.6%\n",
            "Optimization Iteration:   9501, Training Accuracy:  92.2%\n",
            "Optimization Iteration:   9601, Training Accuracy:  96.9%\n",
            "Optimization Iteration:   9701, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   9801, Training Accuracy:  95.3%\n",
            "Optimization Iteration:   9901, Training Accuracy:  98.4%\n",
            "Time usage: 0:05:41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFfdBs5HlH22",
        "outputId": "34b63f01-5ddf-4d71-d263-28ff1437aa9f"
      },
      "source": [
        "print_test_accuracy()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test-Set: 93.0% (9305 / 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dpQnE-HlJWg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}